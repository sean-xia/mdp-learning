# 引言


```大部分译自《From Markov chains to Markov decision processes》```

我们先理解一下增强学习的基本思想。考虑训练狗坐下的过程。狗开始并不懂什么叫坐下，直到有一天当你呵令它坐下时，它偶尔做对了。然后你奖赏给它一块饼干。到后来，狗狗就知道了每当它听到“坐下”的指令时，它照做了会得到一块饼干。这就是增强学习的基本思想。

现在，人们想用同样的办法来训练计算机，让它按照我们的的设计完成一些复杂的工作。

我们感兴趣的这一理论基于马尔科夫理论。通常假设我们有一个时域离散的马尔科夫链（Markov chain），在每一个时间段，它都有一个所谓的**状态**转移概率分布，用来描述这个随机过程状态变化的概率。我们一般称这种转移为“**行动**”，一个单项选择行动，它是每个时间段边缘强制发生的。如果我们考虑一个多项选择行动，那意味着，不同的选项有不同的转移概率。当一次行动发生后，系统转移到下一个状态，我们给予这一过程一定的**奖励**。 

那么问题就可以这样归纳：我们如何找到一个行动集合，称为**策略**，来最大化经历一些过程之后的总奖励。我们把具有这类性质的过程称为马尔科夫决策过程（Markov Decision Process， MDP）


一个MDP问题可以用动态规划理论来求解。动态规划是优化问题中的一个大类，我们将聚焦于逆推算法。动态规划的核心概念是贝尔曼方程。

随着计算机的发展，源自于动态规划的仿真技术变成有力的工具。其中的一个例子是增强学习（RL），它结合了动态规划和蒙特卡洛仿真。本文不会过多关注增强学习家族，而是通过大家较为熟悉的马尔科夫链来引入MDP的概念，然后介绍一类经典的增强学习算法用来解决MDP问题。这个增强学习算法是Q-学习。

本文分为三个部分：
* 马尔科夫链
* 马尔科夫决策过程
* 增强学习

我们的风格不包括理论的完整证明，而是用一种非正式的方式来介绍，但同时给出了如何找到严格证明。对于一些核心的公式给出推导过程。 我们同时用一些简单的例子来解释这些概念，我们发现这些例子对于理解MDP非常有帮助。 当然这些例子和推导都可以跳过而不影响文章的阅读。我们在每一部分的开头会给出部分参考文献，大部分参考文献出自教育类文献而非科研文章。




